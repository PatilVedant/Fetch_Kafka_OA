Kafka Data Pipeline Project
Introduction
This project involves creating a data pipeline using Kafka and Docker. The goal is to ingest streaming data, process it in real-time, and store the processed data into a new Kafka topic. This document provides an in-depth explanation of the design decisions and implementation details of the project.

Project Setup
Docker Setup
Docker is used to ensure a consistent environment setup across different systems. The project uses Docker Compose to set up Kafka, Zookeeper, and a data generator. The docker-compose.yml file specifies the services required:

Zookeeper: Coordinates and manages the Kafka brokers.
Kafka: A distributed event streaming platform used for building real-time data pipelines.
Data Generator: Produces simulated user login data to the user-login Kafka topic.

Kafka Consumer-Producer Script
The core of the data pipeline is implemented in the consumer_producer.py script. This script performs the following tasks:

Consumes data from the user-login Kafka topic.
- Processes the data by transforming the device_type field to uppercase.
- Produces the processed data to the processed-user-login Kafka topic.

1) Kafka Configuration
- Consumer Configuration: Sets up the Kafka consumer to read messages from the user-login topic.
- Producer Configuration: Sets up the Kafka producer to send messages to the processed-user-login topic.

Python Code Below:
consumer_conf = {
    'bootstrap.servers': 'localhost:29092',
    'group.id': 'my-group',
    'auto.offset.reset': 'earliest'
}

producer_conf = {
    'bootstrap.servers': 'localhost:29092'
}

2) Processing Logic
JSON Decoding: When a message is received, it is first decoded from JSON format. If the decoding fails, the script catches the 
json.JSONDecodeError, logs an error, and initializes an empty dictionary to avoid breaking the processing flow.

Code below:
def process_message(message):
    try:
        data = json.loads(message.value().decode('utf-8'))
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON: {e}")
        data = {}

    if 'device_type' in data:
        data['device_type'] = data['device_type'].upper()
    else:
        logger.warning(f"'device_type' key not found in message: {data}")

    return json.dumps(data).encode('utf-8')

3) Batch Processing
To enhance efficiency, the script processes messages in batches. 
The batch size is set to 10, and messages are produced to the processed-user-login topic once the batch size is reached.

Code Below:
batch_size = 10
messages = []

try:
    while True:
        msg = consumer.poll(timeout=1.0)
        if msg is None:
            continue
        if msg.error():
            if msg.error().code() == KafkaError._PARTITION_EOF:
                continue
            else:
                logger.error(msg.error())
                break
        
        processed_message = process_message(msg)
        messages.append(processed_message)

        if len(messages) >= batch_size:
            produce_messages(messages)
            messages = []
except KeyboardInterrupt:
    pass
finally:
    if messages:
        produce_messages(messages)
    consumer.close()

Logging
Logging is used instead of print statements for better performance and to provide more control over log levels and formatting. 
The logging module is configured to log messages at the INFO level.

Code below:
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

Capitalization of device_type Field
The decision to capitalize the device_type field stems from a common data processing requirement to maintain consistency and 
standardization in data formatting. By converting the device_type to uppercase, we ensure that all values for this field are uniform, 
making it easier to analyze and query the data later. This standardization step helps avoid issues that might arise from case-sensitive 
comparisons or aggregations.

Handling Missing device_type Field
If the device_type field is missing from the incoming message, the script logs a warning indicating the absence of this key. 
This approach ensures that such anomalies are recorded for future investigation, allowing data engineers or analysts to address 
potential data quality issues. The processing continues without interruption, which is crucial for maintaining the integrity and
performance of the real-time data pipeline.

